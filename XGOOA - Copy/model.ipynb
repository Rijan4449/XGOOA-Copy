{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37919ec6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import traceback\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "# === 1. LOAD MODEL + PREPROCESSOR + DATA ===\n",
    "general_model = xgb.XGBClassifier()\n",
    "general_model.load_model(\"mooa_xgb_model_v4.json\")\n",
    "\n",
    "general_preprocessor = joblib.load(\"mooa_preprocessor_v4.joblib\")\n",
    "\n",
    "super_dataset = pd.read_csv(\"super_dataset.csv\")\n",
    "\n",
    "# Luzon lake baseline environmental data (updated 2025-10-08)\n",
    "luzon_lakes = pd.DataFrame({\n",
    "    \"Lake Name\": [\n",
    "        \"Laguna_de_Bay\", \"Lake_Taal\", \"Sampaloc_Lake\", \"Yambo_Lake\",\n",
    "        \"Pandin_Lake\", \"Mohicap_Lake\", \"Palakpakin_Lake\", \"Nabao_Lake\",\n",
    "        \"Tadlac_Lake\", \"Tikub_Lake\", \"Lake_Buhi\", \"Lake_Danao\", \"Bunot_Lake\"\n",
    "    ],\n",
    "    \"pH\": [9.12, 8.32, 7.9, 7.9, 7.8, 7.7, 8.0, 6.33, 7.44, 8.08, 7.95, 7.81, 7.2],\n",
    "    \"Salinity (ppt)\": [0.746, 0.85, 0.1, 0.1, 0.1, 0.1, 0.1, 0.25, 0.361, 0.1, 0.7, 0.1, 0.1],\n",
    "    \"Dissolved Oxygen (mg/L)\": [7.54, 5.61, 3.1, 5.0, 7.3, 4.1, 5.0, 3.14, 7.27, 5.53, 6.89, 7.15, 7.7],\n",
    "    \"BOD (mg/L)\": [1.93, 3.82, 8.0, 2.5, 2.0, 6.8, 3.1, 3.0, 2.33, 2.3, 1.76, 2.49, 10.2],\n",
    "    \"Turbidity (NTU)\": [161.88, 28.0, 28.0, 9.8, 6.5, 10.0, 28.0, 3.5, 3.5, 3.5, 6.18, 2.25, 9.0],\n",
    "    \"Temperature (¬∞C)\": [28.5, 25.5, 27.8, 26.5, 25.8, 26.2, 24.2, 28.0, 29.5, 30.4, 28.5, 29.5, 28.5]\n",
    "})\n",
    "\n",
    "# === 2. FEATURE IMPORTANCE ANALYSIS ===\n",
    "def get_feature_importance_plots():\n",
    "    \"\"\"Generate feature importance visualizations.\"\"\"\n",
    "    try:\n",
    "        # Get feature importances from the model\n",
    "        booster = general_model.get_booster()\n",
    "        importance_dict = booster.get_score(importance_type='gain')\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        importance_df = pd.DataFrame(\n",
    "            list(importance_dict.items()),\n",
    "            columns=['Feature', 'Importance']\n",
    "        ).sort_values(by='Importance', ascending=False)\n",
    "        \n",
    "        # Map model features (f0, f1, ...) to preprocessor feature names\n",
    "        try:\n",
    "            feature_names = general_preprocessor.get_feature_names_out()\n",
    "            feature_map = {f\"f{i}\": name for i, name in enumerate(feature_names)}\n",
    "            importance_df[\"Feature\"] = importance_df[\"Feature\"].map(feature_map)\n",
    "            mapping_success = True\n",
    "        except Exception as e:\n",
    "            print(\"‚ö†Ô∏è Could not map feature names:\", e)\n",
    "            mapping_success = False\n",
    "        \n",
    "        # Create two plots\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "        \n",
    "        # Plot 1: XGBoost built-in plot\n",
    "        xgb.plot_importance(general_model, importance_type='gain', max_num_features=20, ax=ax1)\n",
    "        ax1.set_title(\"Top 20 Features (XGBoost Built-in)\", fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Plot 2: Mapped feature names (if successful)\n",
    "        if mapping_success:\n",
    "            top20 = importance_df.head(20).iloc[::-1]\n",
    "            ax2.barh(range(len(top20)), top20[\"Importance\"], color=\"skyblue\")\n",
    "            ax2.set_yticks(range(len(top20)))\n",
    "            ax2.set_yticklabels(top20[\"Feature\"], fontsize=9)\n",
    "            ax2.set_xlabel(\"Gain (Average Improvement)\", fontsize=11)\n",
    "            ax2.set_ylabel(\"Feature\", fontsize=11)\n",
    "            ax2.set_title(\"Top 20 Features (Mapped Names)\", fontsize=14, fontweight='bold')\n",
    "            ax2.grid(axis=\"x\", linestyle=\"--\", alpha=0.6)\n",
    "        else:\n",
    "            ax2.text(0.5, 0.5, \"Feature name mapping failed\", \n",
    "                    ha='center', va='center', fontsize=12)\n",
    "            ax2.set_title(\"Mapped Names (Unavailable)\", fontsize=14)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Convert plot to image\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format='png', dpi=100, bbox_inches='tight')\n",
    "        buf.seek(0)\n",
    "        img = Image.open(buf)\n",
    "        plt.close()\n",
    "        \n",
    "        # Return both image and table\n",
    "        return img, importance_df.head(30)\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Create error image\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.text(0.5, 0.5, f\"Error generating plots:\\n{str(e)}\", \n",
    "               ha='center', va='center', fontsize=12, color='red')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format='png', dpi=100)\n",
    "        buf.seek(0)\n",
    "        img = Image.open(buf)\n",
    "        plt.close()\n",
    "        \n",
    "        error_df = pd.DataFrame([{\"Error\": str(e)}])\n",
    "        return img, error_df\n",
    "\n",
    "\n",
    "# === 3. BUILD INPUT DATAFRAME ===\n",
    "def build_input_dataframe(species_name, temperature, ph, salinity, do, bod, turbidity):\n",
    "    \"\"\"Build input DataFrame using biological + user + lake environmental data.\"\"\"\n",
    "    species_row = super_dataset[super_dataset['species'] == species_name]\n",
    "    if species_row.empty:\n",
    "        raise ValueError(f\"Species '{species_name}' not found in dataset.\")\n",
    "    species_row = species_row.iloc[0].to_dict()\n",
    "\n",
    "    rows = []\n",
    "    for _, lake in luzon_lakes.iterrows():\n",
    "        row = {**species_row}\n",
    "        row.update({\n",
    "            \"waterbody_name\": lake[\"Lake Name\"],\n",
    "            \"wb_ph_min\": lake[\"pH\"],\n",
    "            \"wb_ph_max\": lake[\"pH\"],\n",
    "            \"wb_salinity_min\": lake[\"Salinity (ppt)\"],\n",
    "            \"wb_salinity_max\": lake[\"Salinity (ppt)\"],\n",
    "            \"wb_do_min\": lake[\"Dissolved Oxygen (mg/L)\"],\n",
    "            \"wb_do_max\": lake[\"Dissolved Oxygen (mg/L)\"],\n",
    "            \"wb_bod_min\": lake[\"BOD (mg/L)\"],\n",
    "            \"wb_bod_max\": lake[\"BOD (mg/L)\"],\n",
    "            \"wb_turbidity_min\": lake[\"Turbidity (NTU)\"],\n",
    "            \"wb_turbidity_max\": lake[\"Turbidity (NTU)\"],\n",
    "            \"wb_temp_min\": lake[\"Temperature (¬∞C)\"],\n",
    "            \"wb_temp_max\": lake[\"Temperature (¬∞C)\"],\n",
    "            \"input_temp\": temperature,\n",
    "            \"input_ph\": ph,\n",
    "            \"input_salinity\": salinity,\n",
    "            \"input_do\": do,\n",
    "            \"input_bod\": bod,\n",
    "            \"input_turbidity\": turbidity\n",
    "        })\n",
    "        rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# === 4. PREDICTION FUNCTION ===\n",
    "def predict_invasion_risk_for_lakes(species_name, temperature, ph, salinity, do, bod, turbidity, debug=False):\n",
    "    \"\"\"Predict invasion risk for Luzon lakes (uses model + lake-specific similarity weighting).\"\"\"\n",
    "    \n",
    "    def categorize_risk(score):\n",
    "        \"\"\"Convert numerical score to risk category.\"\"\"\n",
    "        if score < 0.34:\n",
    "            return \"üü¢ Low Risk\"\n",
    "        elif score < 0.67:\n",
    "            return \"üü° Medium Risk\"\n",
    "        else:\n",
    "            return \"üî¥ High Risk\"\n",
    "    \n",
    "    try:\n",
    "        # --- 1. Build base input ---\n",
    "        input_df = build_input_dataframe(species_name, temperature, ph, salinity, do, bod, turbidity)\n",
    "\n",
    "        # --- 2. Compute derived columns required by preprocessor ---\n",
    "        input_df[\"temp_pref_range\"] = input_df[\"temp_pref_max\"] - input_df[\"temp_pref_min\"]\n",
    "        input_df[\"wb_ph_range\"] = input_df[\"wb_ph_max\"] - input_df[\"wb_ph_min\"]\n",
    "        input_df[\"wb_temp_range\"] = input_df[\"wb_temp_max\"] - input_df[\"wb_temp_min\"]\n",
    "\n",
    "        input_df[\"temp_in_pref_range\"] = (\n",
    "            (input_df[\"input_temp\"] >= input_df[\"temp_pref_min\"]) &\n",
    "            (input_df[\"input_temp\"] <= input_df[\"temp_pref_max\"])\n",
    "        ).astype(int)\n",
    "\n",
    "        input_df[\"fish_ph_pref\"] = (input_df[\"wb_ph_min\"] + input_df[\"wb_ph_max\"]) / 2\n",
    "        input_df[\"ph_difference\"] = abs(input_df[\"fish_ph_pref\"] - input_df[\"input_ph\"])\n",
    "\n",
    "        # --- DEBUG OUTPUT ---\n",
    "        if debug:\n",
    "            print(\"\\n=== DEBUGGING INFO ===\")\n",
    "            print(f\"\\nSpecies: {species_name}\")\n",
    "            print(f\"User Inputs: temp={temperature}, pH={ph}, sal={salinity}, DO={do}, BOD={bod}, turb={turbidity}\")\n",
    "            print(f\"\\nSpecies temperature preference: {input_df.iloc[0]['temp_pref_min']}-{input_df.iloc[0]['temp_pref_max']}¬∞C\")\n",
    "            print(f\"Temperature in preference range: {bool(input_df.iloc[0]['temp_in_pref_range'])}\")\n",
    "            print(f\"\\nCreated features: {input_df.columns.tolist()}\")\n",
    "            print(f\"\\nSample row for {input_df.iloc[0]['waterbody_name']}:\")\n",
    "            print(input_df.iloc[0][['waterbody_name', 'wb_ph_min', 'wb_temp_min', 'input_temp', 'input_ph', 'fish_ph_pref', 'ph_difference']])\n",
    "            \n",
    "            try:\n",
    "                expected_features = general_preprocessor.feature_names_in_\n",
    "                missing = set(expected_features) - set(input_df.columns)\n",
    "                extra = set(input_df.columns) - set(expected_features)\n",
    "                if missing:\n",
    "                    print(f\"\\n‚ö†Ô∏è WARNING: Missing features: {missing}\")\n",
    "                if extra:\n",
    "                    print(f\"\\n‚ö†Ô∏è WARNING: Extra features (will be ignored): {extra}\")\n",
    "            except AttributeError:\n",
    "                print(\"\\n‚ö†Ô∏è Preprocessor doesn't have feature_names_in_ attribute\")\n",
    "\n",
    "        # --- 3. Transform + predict ---\n",
    "        try:\n",
    "            X_processed = general_preprocessor.transform(input_df)\n",
    "            y_pred_proba = general_model.predict_proba(X_processed)[:, 1]\n",
    "        except Exception as transform_error:\n",
    "            error_msg = f\"Transform Error: {str(transform_error)}\"\n",
    "            return pd.DataFrame([{\"Lake Name\": \"‚ùå ERROR\", \"Error\": error_msg}])\n",
    "\n",
    "        if debug:\n",
    "            print(f\"\\nRaw predictions: {y_pred_proba}\")\n",
    "\n",
    "        # --- 4. Compute lake-specific similarity weights ---\n",
    "        env_cols = ['pH', 'Salinity (ppt)', 'Dissolved Oxygen (mg/L)', 'BOD (mg/L)', 'Turbidity (NTU)', 'Temperature (¬∞C)']\n",
    "        user_env = np.array([ph, salinity, do, bod, turbidity, temperature])\n",
    "        \n",
    "        similarities = []\n",
    "        for _, lake in luzon_lakes.iterrows():\n",
    "            lake_env = lake[env_cols].values.astype(float)\n",
    "            dist = np.linalg.norm(lake_env - user_env)\n",
    "            sim = np.exp(-dist / 10)\n",
    "            similarities.append(sim)\n",
    "        similarities = np.array(similarities)\n",
    "\n",
    "        if debug:\n",
    "            print(f\"\\nSimilarity weights (how close user input is to each lake):\")\n",
    "            for i, name in enumerate(luzon_lakes[\"Lake Name\"]):\n",
    "                print(f\"  {name}: {similarities[i]:.3f}\")\n",
    "\n",
    "        adjusted_scores_multiply = y_pred_proba * similarities\n",
    "\n",
    "        result_df = pd.DataFrame({\n",
    "            \"Lake Name\": luzon_lakes[\"Lake Name\"],\n",
    "            \"Raw Score\": np.round(y_pred_proba, 3),\n",
    "            \"Raw Risk Level\": [categorize_risk(s) for s in y_pred_proba],\n",
    "            \"Similarity\": np.round(similarities, 3),\n",
    "            \"Adjusted Score\": np.round(adjusted_scores_multiply, 3),\n",
    "            \"Adjusted Risk Level\": [categorize_risk(s) for s in adjusted_scores_multiply]\n",
    "        }).sort_values(by=\"Adjusted Score\", ascending=False)\n",
    "\n",
    "        # --- 5. Warn if input is far from all Luzon lake conditions ---\n",
    "        if similarities.max() < 0.05:\n",
    "            warning = \"‚ö†Ô∏è WARNING: Your inputs are very different from all Luzon lake baselines. Predictions may be unreliable.\"\n",
    "            result_df = pd.concat([\n",
    "                pd.DataFrame([{\n",
    "                    \"Lake Name\": warning,\n",
    "                    \"Raw Score\": 0,\n",
    "                    \"Raw Risk Level\": \"N/A\",\n",
    "                    \"Similarity\": 0,\n",
    "                    \"Adjusted Score\": 0,\n",
    "                    \"Adjusted Risk Level\": \"N/A\"\n",
    "                }]),\n",
    "                result_df\n",
    "            ], ignore_index=True)\n",
    "        \n",
    "        if debug:\n",
    "            print(\"\\n=== RISK INTERPRETATION ===\")\n",
    "            print(\"Thresholds:\")\n",
    "            print(\"  üü¢ Low Risk: 0.00 - 0.33\")\n",
    "            print(\"  üü° Medium Risk: 0.34 - 0.66\")\n",
    "            print(\"  üî¥ High Risk: 0.67 - 1.00\")\n",
    "            print(\"\\nTop 3 lakes by adjusted risk:\")\n",
    "            for i, row in result_df.head(3).iterrows():\n",
    "                if \"WARNING\" not in str(row[\"Lake Name\"]):\n",
    "                    print(f\"  {row['Lake Name']}: {row['Adjusted Score']:.3f} ({row['Adjusted Risk Level']})\")\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        if len(error_msg) > 200:\n",
    "            error_msg = error_msg[:200] + \"...\"\n",
    "        return pd.DataFrame([{\n",
    "            \"Lake Name\": \"‚ùå ERROR\",\n",
    "            \"Error\": error_msg,\n",
    "            \"Details\": \"Check console for full traceback\"\n",
    "        }])\n",
    "\n",
    "\n",
    "# === 5. GRADIO UI ===\n",
    "def gradio_predict(species_name, temperature, ph, salinity, do, bod, turbidity, enable_debug):\n",
    "    return predict_invasion_risk_for_lakes(species_name, temperature, ph, salinity, do, bod, turbidity, debug=enable_debug)\n",
    "\n",
    "species_list = sorted(super_dataset['species'].unique())\n",
    "\n",
    "with gr.Blocks(theme=\"soft\") as demo:\n",
    "    gr.Markdown(\"# üêü Invasive Species Risk Predictor - Luzon Lakes\")\n",
    "    \n",
    "    with gr.Tabs():\n",
    "        # === TAB 1: PREDICTION ===\n",
    "        with gr.Tab(\"üîÆ Risk Prediction\"):\n",
    "            gr.Markdown(\"### Adjust the sliders to simulate lake conditions and see invasion risks.\")\n",
    "            gr.Markdown(\"**Debug mode prints detailed info to console**\")\n",
    "\n",
    "            with gr.Row():\n",
    "                species_dropdown = gr.Dropdown(label=\"Species\", choices=species_list, value=species_list[0])\n",
    "                debug_checkbox = gr.Checkbox(label=\"Enable Debug Output (check console)\", value=False)\n",
    "\n",
    "            with gr.Row():\n",
    "                temp_slider = gr.Slider(0, 40, 27.0, label=\"Temperature (¬∞C)\")\n",
    "                ph_slider = gr.Slider(0, 14, 7.0, label=\"pH\")\n",
    "                salinity_slider = gr.Slider(0, 10, 0.1, label=\"Salinity (ppt)\")\n",
    "\n",
    "            with gr.Row():\n",
    "                do_slider = gr.Slider(0, 15, 5.0, label=\"Dissolved Oxygen (mg/L)\")\n",
    "                bod_slider = gr.Slider(0, 20, 5.0, label=\"BOD (mg/L)\")\n",
    "                turbidity_slider = gr.Slider(0, 500, 100, label=\"Turbidity (NTU)\")\n",
    "\n",
    "            predict_btn = gr.Button(\"üîÆ Predict Invasion Risk\", variant=\"primary\")\n",
    "            \n",
    "            gr.Markdown(\"### Results Explanation:\")\n",
    "            gr.Markdown(\"\"\"\n",
    "            - **Raw Score**: Direct XGBoost model prediction (0-1 probability)\n",
    "            - **Similarity**: How close your inputs match each lake's baseline (1=exact, 0=very different)\n",
    "            - **Adjusted Score**: Raw prediction √ó similarity (penalizes dissimilar conditions)\n",
    "            - **Risk Categories**: üü¢ Low (0-0.33) | üü° Medium (0.34-0.66) | üî¥ High (0.67-1.00)\n",
    "            \"\"\")\n",
    "            \n",
    "            output_table = gr.Dataframe()\n",
    "\n",
    "            predict_btn.click(\n",
    "                fn=gradio_predict,\n",
    "                inputs=[species_dropdown, temp_slider, ph_slider, salinity_slider, do_slider, bod_slider, turbidity_slider, debug_checkbox],\n",
    "                outputs=output_table\n",
    "            )\n",
    "\n",
    "            # Add test scenarios\n",
    "            gr.Markdown(\"### üß™ Quick Test Scenarios:\")\n",
    "            with gr.Row():\n",
    "                test1_btn = gr.Button(\"Test 1: Match Lake Taal\")\n",
    "                test2_btn = gr.Button(\"Test 2: Extreme Values\")\n",
    "                test3_btn = gr.Button(\"Test 3: Optimal Conditions\")\n",
    "            \n",
    "            def test_lake_taal():\n",
    "                return 25.5, 8.32, 0.85, 5.61, 3.82, 28.0\n",
    "            \n",
    "            def test_extreme():\n",
    "                return 5.0, 3.0, 8.0, 2.0, 15.0, 400.0\n",
    "            \n",
    "            def test_optimal():\n",
    "                return 26.0, 7.2, 0.2, 8.0, 2.0, 10.0\n",
    "            \n",
    "            test1_btn.click(fn=test_lake_taal, outputs=[temp_slider, ph_slider, salinity_slider, do_slider, bod_slider, turbidity_slider])\n",
    "            test2_btn.click(fn=test_extreme, outputs=[temp_slider, ph_slider, salinity_slider, do_slider, bod_slider, turbidity_slider])\n",
    "            test3_btn.click(fn=test_optimal, outputs=[temp_slider, ph_slider, salinity_slider, do_slider, bod_slider, turbidity_slider])\n",
    "        \n",
    "        # === TAB 2: FEATURE IMPORTANCE ===\n",
    "        with gr.Tab(\"üìä Feature Importance\"):\n",
    "            gr.Markdown(\"### Understanding What Drives the Model's Predictions\")\n",
    "            gr.Markdown(\"\"\"\n",
    "            This analysis shows which features have the most impact on the model's invasion risk predictions.\n",
    "            **Gain** measures the average improvement in prediction accuracy when using each feature.\n",
    "            \"\"\")\n",
    "            \n",
    "            analyze_btn = gr.Button(\"üìà Generate Feature Importance Analysis\", variant=\"primary\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                importance_plot = gr.Image(label=\"Feature Importance Visualization\")\n",
    "            \n",
    "            importance_table = gr.Dataframe(label=\"Top 30 Most Important Features\")\n",
    "            \n",
    "            gr.Markdown(\"\"\"\n",
    "            **How to interpret:**\n",
    "            - Higher gain = more important for predictions\n",
    "            - Features at the top have the strongest influence on risk assessment\n",
    "            - Compare both visualizations to understand feature naming conventions\n",
    "            \"\"\")\n",
    "            \n",
    "            analyze_btn.click(\n",
    "                fn=get_feature_importance_plots,\n",
    "                inputs=[],\n",
    "                outputs=[importance_plot, importance_table]\n",
    "            )\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
